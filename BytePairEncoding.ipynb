{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccwbroomfield/NLP_work/blob/main/BytePairEncoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E4Gfg4j_mjn"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset_train = load_dataset('imdb', split=\"train\")\n",
        "dataset_test = load_dataset('imdb', split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jme9iULt34WR"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from collections import Counter\n",
        "def learn(text: List[str], k: int):\n",
        "  print(\"learn method\")\n",
        "  words = \"\"\n",
        "  \n",
        "  for i in range(len(text)):      #combine text entries, replace spaces with _\n",
        "    words += text[i] + \" \"\n",
        "\n",
        "  word_dict = words.split(\" \")\n",
        "\n",
        "  count_dict = Counter()  #the number of times each word appears in words\n",
        "  for i in word_dict:\n",
        "    count_dict[tuple(list(i + \"_\"))] += 1   #count occurences of word, store as list of chars with \"_\" at end\n",
        "                                            #storing as tuple so i can use like list, but hashable\n",
        "\n",
        "  words = words.replace(\" \", \"_\")\n",
        "  alphabet = list(words)\n",
        "  alphabet = set(alphabet)  #get distinct chars\n",
        "  \n",
        "  count = Counter()    #the number of times each character appears next to another\n",
        "\n",
        "  for word in list(count_dict):         #get initial count of tokens\n",
        "    for c in range(len(word) - 1):\n",
        "      if \"_\" not in word[c]:\n",
        "        count[word[c] + word[c+1]] += count_dict[word]   #not just 1 since this word appears count_dict[word] number of times\n",
        "\n",
        "\n",
        "  disc_tokens = [] # keep track of all new tokens after original alphabet\n",
        "  for i in range(k):   #main loop to get k tokens\n",
        "    token = \"\"\n",
        "\n",
        "    j = 1\n",
        "    while True:  #get the most common token that hasnt been used already\n",
        "      if (count.most_common(j)[j-1][0] in disc_tokens):\n",
        "        j+=1\n",
        "      else:\n",
        "        token = count.most_common(j)[j-1][0]\n",
        "        break\n",
        "    \n",
        "    alphabet.add(token)\n",
        "    disc_tokens.append(token)  #add the token to discovered tokens and alphabet\n",
        "\n",
        "\n",
        "    new_count_dict = Counter()\n",
        "    for word in list(count_dict):            #go through every word in the corpus\n",
        "      new_word = []\n",
        "      k=0\n",
        "      while k < len(word):             #go through every character, replacing pairs of tokens with new token\n",
        "        if k < len(word) - 1 and token == word[k] + word[k+1]:\n",
        "          new_word.append(token)\n",
        "          if k <len(word) - 2 and \"_\" not in token:    #if there's space, get the count of the next possible token and add it to count. doesn't change dict\n",
        "            count[word[k] + word[k + 1] + words[k + 2]] += count_dict[word]\n",
        "          if k > 0 and \"_\" not in word[k-1]: #need to look for tokens where thing before is being merged with our token too\n",
        "            count[word[k-1] + word[k] + words[k+1]] += count_dict[word]\n",
        "          k += 1\n",
        "        else:    #if not the token just add it back and don't skip index\n",
        "          new_word.append(word[k])\n",
        "        k += 1\n",
        "      new_count_dict[tuple(new_word)] = count_dict[word]   #after while loop, add the new word to new count dict with num occurrences\n",
        "\n",
        "    count_dict = new_count_dict    #after for loop, update our count_dict so it has all the new words with tokens replaced\n",
        "\n",
        "  return disc_tokens    #return the k tokens found. Could also return alphabet here if preferred\n",
        "\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzNA9usmRRX0"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "def segment(text: str, tokens: List[str]):\n",
        "  print(\"segment method\")\n",
        "\n",
        "  words = text\n",
        "  words = words.replace(\" \", \"_\")\n",
        "  words = list(words)\n",
        "\n",
        "  for i in tokens:\n",
        "    new_text = []\n",
        "    j = 0\n",
        "    while j < len(words):\n",
        "      if j < len(words) - 1 and (i == words[j] + words[j + 1]):\n",
        "        new_text.append(i)\n",
        "        j += 1\n",
        "      else:\n",
        "        new_text.append(words[j])\n",
        "      j += 1\n",
        "    words = new_text\n",
        "  return words\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8coTOcXUHTil"
      },
      "source": [
        "Heap's/Herdan's law predicts that the number of distinct words in a corpus of text will grow as Kn^Î’, where K is generall between 10-100 and B is between .4 and .6 for English collections. n is the number of words in the text. For the imdb datasets, n is 32,369,810. \n",
        "This means that we might expect the number of distinct words in the corpus to be around 50 * 32,369,810^.5 = 284,472. On the low end we might expect 10 * 32,369,810^.4 = 10,093. \n",
        "\n",
        "> The first 30 new tokens found were 'e_', 's_', 'th', 'he', 't_', 'in', 'd_', 'er', 'an', 'n_', 'r_', 're', 'y_', 'is', 'the_', ',_', 'nd', 'en', 'on', 'at', '._', 'o_', 'ng', 'or', 'es', 'it', 'ha', 'to', 'st', 'ou'. After significant optimization, this computation took 59s to complete. To find all ~285,000 would take considerable time, and exceed the colab limit. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OkC_HG5fhcuQ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize, WhitespaceTokenizer\n",
        "nltk.download('punkt')\n",
        "words = \"\"\n",
        "text = dataset_test[\"text\"]\n",
        "for i in range(len(text)):      #combine text entries, replace spaces with _\n",
        "    words += text[i] + \" \"\n",
        "\n",
        "tok = learn(dataset_train[\"text\"], 1000)   \n",
        "l_BP = segment(words, tok)\n",
        "l_wt = word_tokenize(words)\n",
        "l_wp = wordpunct_tokenize(words)\n",
        "list(WhitespaceTokenizer().span_tokenize(words))\n",
        "print(\"My BPE tokenizer broke the test data into \" + str(len(l_BP)) + \" tokens while the nltk word_tokenize method returned \" + str(len(l_wt)) + \" words\")\n",
        "print (\" and the wordpunct_tokenize function returned \" + str(len(l_wp)) + \" words.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPKWJbz_MWuI"
      },
      "source": [
        "**Output**: My BPE tokenizer broke the test data into 17,188,243 tokens while the nltk word_tokenize method returned 6,900,297 words\n",
        " and the wordpunct_tokenize function returned 6,941,137 words. The computation completed in 3h 38m 45s.\n",
        "\n",
        "> The much larger number of tokens, 17.2 million vs 6.9, is because my tokens were limited to the top 1000. This means that most of the tokens are still word fragments, and will thus exceed the number of words in the testing data (which is 5.7 million). The other tokenizers also exceed this because they treat punctuation and other symbols as their own words, but are much closer to the 5.7 million words. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0t8AI_vgSDk"
      },
      "source": [
        "The nltk tokenizers seem much better at actually extracting real, meaningful, and useful words rather than the simple pairings that BPE generates first. I imagine this allows BPE to ensure that it can deal with words it has not seen in training in a way that I'm not sure the nltk tokenizers can do, but it seems that BPE has to 'brute-force' things a bit more.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "When running BPE the first things I get are word fragments, like common ends of words or the ever-present \"th\" pairing. The nltk tokenizers give me a list of the actual words, since it appears they just break along whitespace. Does this make BPE less language specific? I imagine that, for languages that cannot be so easily broken along whitespace as english, BPE could still add significant value in breaking down the text.\n",
        "\n",
        "> I was a bit unclear as to what the Whitespace_Tokenizer actually does since I'm unfamiliar with token-span. The output did not seem to correlate with the BPE or other nltk tokenizers. \n",
        "\n",
        "Another big difference was time. I am not sure if it is the algorithm, advanced implementation on nltk's end, or shoddy implementation on my end, but far more information is gleaned per unit of time from the built in tokenizers. To find the first 30 tokens using my implementation of BPE, which includes only the simplest words like \"the\" and \"an\", about a minute of computation was necessary. \n",
        "\n",
        "---\n",
        "\n",
        "wordpunct_tokenize required only 2 seconds to complete, and returns what would immediately appear to be more useful data. word_tokenize needed 43 seconds, excruciating when compared to wordpunct_tokenize but far faster, with far more useful words returned, than my BPE tokenizer implementation. The overlap between the BPE tokenizer and the word(punct)_tokenizer is minimal, including only the shortest of common stop words. The vast majority of the tokens returned by BPE are 2-4 character combinations that make up words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwI4BAKttjYc"
      },
      "source": [
        "I believe BPE could likely be enhanced in a couple of ways. For one, there is likely room to optimize my own implementation. \n",
        "However, beyond that, I believe the other nltk tokenizers see much improved performance for a few reasons\n",
        "\n",
        "1.   **Whitespace as a useful separator**. While I'm not sure how unseen words are managed on these tokenizers, the immediate return of words, rather than tokens built from the ground up by the BPE tokenizer, seems far more efficient and is my guess as to why better performance is seen in less time\n",
        "2.   **Better punctuation management**. The nltk tokenizers have punctuation broken out usefully into separate words. BPE only associates punctuation with previous characters rather than distinct features of their own. Breaking these symbols out into their own separate representation would likely improve utility.\n",
        "3. **Top-down approach**. I wonder if tokenizer algorithms could be enhanced by only performning BPE on the text that is 'left over', or not recnignized, after extracting known words and punctuation immediately. This seems like it would eliminate the amount of text that would have to be built from the bottom-up, and the text that is immediately recognized as words or symbols by basic white-space processing could be identified faster\n",
        "\n",
        " "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BytePairEncoding.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPAR9xh+MXFC+PTCKs3ak9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}