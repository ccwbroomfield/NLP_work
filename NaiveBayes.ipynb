{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2kkGuTVI1MnI5PRBaeJ3K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccwbroomfield/NLP_work/blob/main/NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40tseCMzE7ly"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset, load_dataset_builder\n",
        "dataset_imdb_train = load_dataset('imdb', split=\"train\")\n",
        "dataset_imdb_test = load_dataset('imdb', split=\"test\")\n",
        "dataset_builder = load_dataset_builder('imdb')\n",
        "print(dataset_builder.info.features)\n",
        "dataset_sms_train = load_dataset('sms_spam', split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import nltk\n",
        "\n",
        "##### FEATURIZERS  ########\n",
        "def bag_of_words(cleaned : List[str]):   #given method\n",
        "  words = {w.lower() for w in cleaned if w.isalpha()}\n",
        "  return list(words)\n",
        "\n",
        "def bag_of_counted_words(tokens: List[str]):\n",
        "  feature_list = []\n",
        "  for w in tokens:\n",
        "    if w.isalpha():\n",
        "      feature_list.append(w)\n",
        "  return feature_list\n",
        "\n",
        "def text_len(tokens: List[str]):   #number of words. would like to classify as long or short but that doesn't generalize\n",
        "  return [len(tokens)]\n",
        "\n",
        "def word_length(tokens: List[str]):   #avg length of word, int so that it'll match more often\n",
        "  avg = 0\n",
        "  for t in tokens:\n",
        "    avg += len(t)\n",
        "  return [int(avg/len(tokens))]\n",
        "\n",
        "def part_of_speech(tokens: List[str]): #treats the parts of speech like words, will get count\n",
        "  text = nltk.pos_tag(tokens)          #from https://www.nltk.org/book/ch05.html\n",
        "  parts_of_speech = []\n",
        "  for word, lab in text:\n",
        "    parts_of_speech.append(lab)\n",
        "\n",
        "  return parts_of_speech\n",
        "\n",
        "def bigrams(tokens: List[str]):    #from https://www.nltk.org/book/ch05.html\n",
        "  return list(nltk.bigrams(tokens))\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "qSdnec1THSsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize # from https://www.nltk.org/api/nltk.tokenize.html\n",
        "import nltk\n",
        "nltk.download('punkt')  #recommended by error message\n",
        "\n",
        "\n",
        "##### CLEANERS ########\n",
        "def basic_split(document: str):\n",
        "  tokens = document.split(\" \")\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def nltk_word_tok(document: str):\n",
        "  return word_tokenize(document)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awJLgp7BH-20",
        "outputId": "0aa08dc7-78fb-4ce9-daca-82501ee6918e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, List, Tuple\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')  #recommended by error message\n",
        "\n",
        "def learn(data, clean, featurize):\n",
        "  prob_table = {}\n",
        "  count_classes = Counter()\n",
        "  #count_features = Counter()\n",
        "  tot_vocab = []\n",
        "  #num_docs = len(data)\n",
        "  \n",
        "  for document, cls in data: # iterate over the data\n",
        "    if cls not in prob_table:\n",
        "      prob_table[cls] = Counter()   #if new class, add new class to nested dict of features\n",
        "    count_classes[cls] += 1  #keep track of how many times we see each class\n",
        "    document = clean(document) # clean the document\n",
        "    features = []\n",
        "    for featurizer in featurize:\n",
        "      features += featurizer(document)\n",
        "    for f in features:\n",
        "      prob_table[cls][f] += 1    #keep track of how many times we see each feature per class\n",
        "      tot_vocab.append(f)   #add to total vocab\n",
        "  \n",
        "\n",
        "  num_docs = len(data)\n",
        "  tot_vocab = set(tot_vocab)     #convert to set so that we can get length of vocab\n",
        "\n",
        "  model = (num_docs, tot_vocab, count_classes, prob_table)\n",
        "  return model\n",
        "  "
      ],
      "metadata": {
        "id": "6ihzD-Ei5lSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aae2c06-b8dc-4efa-ff0d-d05ab7867e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "def classify(text, model, clean, featurize):\n",
        "  num_docs = model[0]\n",
        "  tot_vocab = model[1]\n",
        "  count_classes = model[2]\n",
        "  prob_table = model[3]\n",
        "  \n",
        "  length_v = len(tot_vocab)\n",
        "\n",
        "  text = clean(text)\n",
        "  features = []\n",
        "  for featurizer in featurize:\n",
        "    features += featurizer(text)\n",
        "  \n",
        "  stripped_text = []\n",
        "  for word in features:       #get rid of words not seen in training\n",
        "    if word not in tot_vocab:\n",
        "      continue\n",
        "    stripped_text.append(word)\n",
        "  \n",
        "  prob = -math.inf\n",
        "  most_prob_class = None\n",
        "\n",
        "  for cls in prob_table:\n",
        "    word_prob = []\n",
        "    prob_class = math.log(count_classes[cls] / num_docs)   # get the probability of the class\n",
        "    #print(\"prob_class of class \" + str(cls) + \" is \" + str(prob_class))\n",
        "    for word in stripped_text:\n",
        "      word_prob.append(prob_table[cls][word] + 1)  #laplace smoothing, number of times each word seen\n",
        "    loc_prob = 1\n",
        "    for num in word_prob:\n",
        "      loc_prob += math.log(num / (len(prob_table[cls]) + length_v))   #sum log of every probability\n",
        "    #print(\"loc_prob of class \" + str(cls) + \" is \" + str(loc_prob))\n",
        "    tot_prob = prob_class + loc_prob #sum total probability\n",
        "    #print(\"tot_prob for class \" + str(cls) + \" is \" + str(tot_prob))\n",
        "    if tot_prob > prob:\n",
        "      prob = tot_prob\n",
        "      most_prob_class = cls\n",
        "  \n",
        "  return most_prob_class"
      ],
      "metadata": {
        "id": "q4G477476F9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def classify_baseline(text, model):\n",
        "  num_docs = model[0]\n",
        "  count_classes = model[2]\n",
        "  prob_table = model[3]\n",
        "\n",
        "  prob = 0\n",
        "  most_prob_class = None\n",
        "\n",
        "  for cls in prob_table:\n",
        "    prob_class = count_classes[cls] / num_docs   # get the probability of the class\n",
        "    #print(\"class \" + str(cls) + \" has a probability of \" + str(prob_class) + \" with \" + str(count_classes[cls]) + \" of the \" + str(num_docs) + \" documents\")\n",
        "    if prob_class > prob:\n",
        "      prob = prob_class\n",
        "      most_prob_class = cls\n",
        "  \n",
        "  return most_prob_class"
      ],
      "metadata": {
        "id": "0ZKspuoF1nji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "cleaner = word_tokenize   #setting cleaner\n",
        "featurize = []   #selectng featurizers\n",
        "featurize.append(bag_of_counted_words)\n",
        "#featurize.append(text_len)\n",
        "#featurize.append(word_length)\n",
        "#featurize.append(part_of_speech)\n",
        "featurize.append(bigrams)\n",
        "\n",
        "data = []  #selecting training data and building data for learn\n",
        "#text = dataset_imdb_train[\"text\"]\n",
        "#labels = dataset_imdb_train[\"label\"]\n",
        "\n",
        "text = dataset_sms_train[\"sms\"]\n",
        "labels = dataset_sms_train[\"label\"]\n",
        "#print(labels)\n",
        "lab_counter = 0\n",
        "for doc in text:    #gotta be a better way but i was having trouble so i matched manually\n",
        "  #if(len(labels) > lab_counter):\n",
        "  data.append((doc, labels[lab_counter]))\n",
        "  lab_counter += 1 \n",
        "\n",
        "\n",
        "## ONLY FOR SMS, BUILDING TRAIN AND TEST SETS ########\n",
        "train_data = []\n",
        "test_data = []\n",
        "random.shuffle(data)\n",
        "len_data = len(data)\n",
        "for index, tup in enumerate(data):   #get a random percentage for testing \n",
        "  if index / len_data < 0.95:\n",
        "    train_data.append(tup)\n",
        "  else:\n",
        "    test_data.append(tup)\n",
        "\n",
        "model = learn(train_data, cleaner, featurize)"
      ],
      "metadata": {
        "id": "RmOLvX-O24JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "accuracy = datasets.load_metric(\"accuracy\")\n",
        "precision = datasets.load_metric(\"precision\")\n",
        "recall = datasets.load_metric(\"recall\")\n",
        "\n",
        "print(\"Running the word_tokenize cleaner and bag_of_counted_words and bigram featurizers, trained on 95% of the sms dataset:\")\n",
        "\n",
        "results_classify = []           #storing how each classifier classified each document in training set\n",
        "results_classify_baseline = []\n",
        "#for text in dataset_imdb_test[\"text\"]: #different for loop depending on testing set, classifying everything in test set\n",
        "for text, lab in test_data:\n",
        "  results_classify.append(classify(text, model, cleaner, featurize))\n",
        "  results_classify_baseline.append(classify_baseline(text, model))\n",
        "\n",
        "#print(results_classify)\n",
        "#print(results_classify_baseline)\n",
        "real_tags = []\n",
        "#for lab in dataset_imdb_test[\"label\"]:\n",
        "for doc, lab in test_data:\n",
        "  real_tags.append(lab)\n",
        "\n",
        "#calculating metrics\n",
        "acc_class = accuracy.compute(predictions = results_classify, references = real_tags)\n",
        "acc_base = accuracy.compute(predictions = results_classify_baseline, references = real_tags)\n",
        "prec_class = precision.compute(predictions = results_classify, references = real_tags)\n",
        "prec_base = precision.compute(predictions = results_classify_baseline, references = real_tags)\n",
        "rec_class = recall.compute(predictions = results_classify, references = real_tags)\n",
        "rec_base = recall.compute(predictions = results_classify_baseline, references = real_tags)\n",
        "\n",
        "f_score_class = (2 * prec_class[\"precision\"] * rec_class[\"recall\"]) / (prec_class[\"precision\"] + rec_class[\"recall\"])\n",
        "#f_score_base = (2 * prec_base[\"precision\"] * rec_base[\"recall\"]) / (prec_base[\"precision\"] + rec_base[\"recall\"])\n",
        "print(\"The accuracy of the naive bayes classifier was \" + str(acc_class[\"accuracy\"]) + \" compared to the baseline of \" + str(acc_base[\"accuracy\"]))\n",
        "print(\"The precision of the naive bayes classifier was \" + str(prec_class[\"precision\"]) + \" compared to the baseline of \" + str(prec_base[\"precision\"]))\n",
        "print(\"The recall of the naive bayes classifier was \" + str(rec_class[\"recall\"]) + \" compared to the baseline of \" + str(rec_base[\"recall\"]))\n",
        "print(\"This gives an F1 score for the naive bayes classifier of \" + str(f_score_class))\n",
        "#print(\" compared to the baseline score of \" + str(f_score_base))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPaL2cNAJgUe",
        "outputId": "4e6531f7-3332-4f94-df94-6eb177b9258b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the word_tokenize cleaner and bag_of_counted_words and bigram featurizers, trained on 95% of the sms dataset:\n",
            "The accuracy of the naive bayes classifier was 0.9820143884892086 compared to the baseline of 0.8345323741007195\n",
            "The precision of the naive bayes classifier was 1.0 compared to the baseline of 0.0\n",
            "The recall of the naive bayes classifier was 0.8913043478260869 compared to the baseline of 0.0\n",
            "This gives an F1 score for the naive bayes classifier of 0.9425287356321839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMDB DATASET**\n",
        "# Results\n",
        "Running the word_tokenize cleaner and bag_of_counted_words featurizer: \n",
        "*   The accuracy of the naive bayes classifier was 0.8198\n",
        "*   The precision of the naive bayes classifier was 0.8217 \n",
        "*   The recall of the naive bayes classifier was 0.8170 \n",
        "*   This gives an F1 score for the naive bayes classifier of 0.8193\n",
        "\n",
        "<br/>Running the word_tokenize cleaner and bag_of_counted_words and bigrams featurizers:\n",
        "*   The accuracy of the naive bayes classifier was 0.8512\n",
        "*   The precision of the naive bayes classifier was 0.8827 \n",
        "*   The recall of the naive bayes classifier was 0.8101\n",
        "* This gives an F1 score for the naive bayes classifier of 0.8449\n",
        "\n",
        "\n",
        "<br />Running the word_tokenize cleaner and text_len, word_length, and part_of_speech featurizers:\n",
        "*   The accuracy of the naive bayes classifier was 0.5909\n",
        "*   The precision of the naive bayes classifier was 0.5884 \n",
        "*   The recall of the naive bayes classifier was 0.6051\n",
        "*   This gives an F1 score for the naive bayes classifier of 0.5966\n",
        "\n",
        "<br />Running the word_tokenize cleaner and bag_of_counted_words, text_len, word_length, part_of_speech, and bigrams featurizers:\n",
        "*   The accuracy of the naive bayes classifier was 0.8462\n",
        "*   The precision of the naive bayes classifier was 0.8783\n",
        "*   The recall of the naive bayes classifier was 0.8038\n",
        "*   This gives an F1 score for the naive bayes classifier of 0.8394\n",
        "\n",
        "<br />For all cases the accuracy of the baseline classifier was 0.5, and the precision and recall were 0. This is because there were an equal number of positive and negative reviews in the testing set, and everything was categorized as 0 (meaning there were no true positives). With a precision and recall of 0, the F1 score is useless and creates a divide by 0 error.\n",
        "\n",
        "---\n",
        "\n",
        "# Analysis\n",
        "\n",
        "Using just the bag_of_counted_words tokenizer, performance on all metrics was approximately 0.82. This seems reliable, consistent, and unbiased. \n",
        "<br/>By including the bigrams featurizer with bag of counted words the accuracy and precision is increased to .85 and .88 respectively. This was the best performing combination of featurizers that I tested. Recall does stay approximately the same, meaning that  the number of things falsely labelled as positive decreased (if the number of true positives increased, recall would have changed). \n",
        "<br/>Classifying simply on text features, the word length, review length, and parts of speech, is dismal (which did not surprise me) and including all 5 featurizers sees a slight dip in performance, but is similar to the bag of counted words and bigrams. \n",
        "\n",
        "<br/>**Future Work**\n",
        "<br/>Future work could still test other combinations of featurizers; I did not exhaust the permutations. I would also be curious if including 3-grams would increase performance as including bigrams did, and what featurizers could increase recall.\n",
        " \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P2USdzta5RiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **SMS-SPAM DATASET**\n",
        "# Results\n",
        "**Running on a random training set of 75% of the data**\n",
        "<br/>Running the word_tokenize cleaner and bag_of_counted_words featurizer:\n",
        "*   The accuracy of the naive bayes classifier was 0.9584 compared to the baseline of 0.8672\n",
        "*   The precision of the naive bayes classifier was 0.9922\n",
        "*   The recall of the naive bayes classifier was 0.6919\n",
        "*   This gives an F1 score for the naive bayes classifier of 0.8153\n",
        "\n",
        "<br/>Running the word_tokenize cleaner and bag_of_counted_words and bigrams featurizers:\n",
        "*   The accuracy of the naive bayes classifier was 0.9785 compared to the baseline of 0.8636\n",
        "*   The precision of the naive bayes classifier was 0.9938\n",
        "*   The recall of the naive bayes classifier was 0.8474\n",
        "*   This gives an F1 score for the naive bayes classifier of 0.9148\n",
        "\n",
        "<br/>Running the word_tokenize cleaner and text_len, word_length, and part_of_speech featurizers:\n",
        "*   The accuracy of the naive bayes classifier was 0.8557 compared to the baseline of 0.8557\n",
        "*   The precision of the naive bayes classifier was 0.0\n",
        "*   The recall of the naive bayes classifier was 0.0\n",
        "\n",
        "*Note: it appears that, when classified based on text properties, the naive bayes classifier behaved exactly like the baseline classifier*\n",
        "\n",
        "<br/>Running the word_tokenize cleaner and bag_of_counted_words, text_len, word_length, part_of_speech, and bigrams featurizers:\n",
        "*   The accuracy of the naive bayes classifier was 0.9512 compared to the baseline of 0.8586\n",
        "*   The precision of the naive bayes classifier was 1.0\n",
        "*   The recall of the naive bayes classifier was 0.6548\n",
        "*   This gives an F1 score for the naive bayes classifier of 0.7914\n",
        "\n",
        "<br>**To assess the performance of different training sizes, the most successful set of featurizers, the bag of counted words and bigrams, were used**\n",
        "\n",
        "<br/>Trained on 95% of the sms dataset:\n",
        "*   The accuracy of the naive bayes classifier was 0.9820 compared to the baseline of 0.8345\n",
        "*   The precision of the naive bayes classifier was 1.0\n",
        "*   The recall of the naive bayes classifier was 0.8913\n",
        "*   This gives an F1 score for the naive bayes classifier of 0.9425\n",
        "\n",
        "<br/>Trained on 90% of the sms dataset:\n",
        "*   The accuracy of the naive bayes classifier was 0.9964 compared to the baseline of 0.8743\n",
        "*   The precision of the naive bayes classifier was 1.0\n",
        "*   The recall of the naive bayes classifier was 0.9714\n",
        "*   This gives an F1 score for the naive bayes classifier of 0.9855\n",
        "\n",
        "<br/>Trained on 80% of the sms dataset:\n",
        "*   The accuracy of the naive bayes classifier was 0.9776 compared to the baseline of 0.8627\n",
        "*   The precision of the naive bayes classifier was 0.9923\n",
        "*   The recall of the naive bayes classifier was 0.8431\n",
        "*   This gives an F1 score for the naive bayes classifier of 0.9117\n",
        "\n",
        "<br/>Trained on 70% of the sms dataset:\n",
        "*   The accuracy of the naive bayes classifier was 0.9743 compared to the baseline of 0.8666\n",
        "*   The precision of the naive bayes classifier was 0.9945\n",
        "*   The recall of the naive bayes classifier was 0.8117\n",
        "*   This gives an F1 score for the naive bayes classifier of 0.8938\n",
        "\n",
        "<br/>Trained on 50% of the sms dataset:\n",
        "*   The accuracy of the naive bayes classifier was 0.9706 compared to the baseline of 0.8629\n",
        "*   The precision of the naive bayes classifier was 0.9902\n",
        "*   The recall of the naive bayes classifier was 0.7932\n",
        "*   This gives an F1 score for the naive bayes classifier of 0.8808\n",
        "\n",
        "<br/> For all cases the precision and recall of the baseline classifier was 0, making the F1 score useless\n",
        "\n",
        "---\n",
        "# Analysis\n",
        "\n",
        "Performance was far better, at least in the best cases on the sms_spam dataset than the imdb dataset. Using just the bag of counted words featurizer, accuracy was 0.96 and precision 0.99, although I was surprised to see a very low recall of 0.69. I imagine this represents the features of the dataset. Adding the bigram featurizer makes marginal improvements to the accuracy and precision but makes dramatic improvements to recall. The addition of the bigram featurizer this bumps the F1 score from 0.81 to .91.\n",
        "<br/>I was a bit surprised by my findings with different featurizers. Using exclusively text data the naive bayes classifer appears to operate in the exact same way as the baseline classifier, but using all featurizers bumps precision to a perfect 1.0 while recall drops to a mere 0.65.\n",
        "<br/> To assess the effects of different training sets, I also ran the best performing combination of featurizers, the bag of counted words and bigrams, with models built from training and testing sets of different percentages of the total data. Interestingly, the optimal training set size appeared to be 90% of the data. \n",
        "<br/>Judged off of F1 scores alone:\n",
        "\n",
        "*   Training on 50% of the data produced an F1 score of 0.8808\n",
        "*   Training on 70% of the data produced an F1 score of 0.8938\n",
        "*   Training on 75% of the data produced an F1 score of 0.9148\n",
        "*   Training on 80% of the data produced an F1 score of 0.9117\n",
        "*   Training on 90% of the data produced an F1 score of 0.9855\n",
        "*   Training on 95% of the data produced an F1 score of 0.9425\n",
        "\n",
        "<br/>**Future Work**\n",
        "<br/>Future work could include different combinations of featurizers, and different featurizers, as the imdb evaluation could. As the sms_spam dataset is not balanced I am especially interested in the precision and recall tradeoffs, as improvements in one metric does not appear to always correspond with an improvement in the other.\n",
        "<br/>Another important area of work could be evaluating the average performance of the model. The training and testing sets were not made permanent, and were randomly created with each model. While multiple tests appeared to return similar results, it may be interesting to average the metrics of the same models run on many different splits of the data. The variability can be seen in the performance of the baseline classifier, which ranged from .83 to .87. Additionally, training on 75% of the data perfomed better than training on 80% of the data, but far worse than training on 90% of the data. I would expect the F1 scores to appear a little more linear with a testing of averages. \n",
        "\n"
      ],
      "metadata": {
        "id": "O6sa2cam_I7Y"
      }
    }
  ]
}